from rest_framework import status, permissions, viewsets
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated
from rest_framework.decorators import action
from django.db.models import Q
from django.utils import timezone
from .models import LLMConfig, ChatSession, ChatMessage
from .serializers import LLMConfigSerializer
import logging
from asgiref.sync import sync_to_async

logger = logging.getLogger(__name__)

# é¡¹ç›®ç›¸å…³å¯¼å…¥
from projects.models import Project, ProjectMember
from projects.permissions import IsProjectMember

# å¯¼å…¥ç»Ÿä¸€çš„æƒé™ç³»ç»Ÿ
from wharttest_django.viewsets import BaseModelViewSet
from wharttest_django.permissions import HasModelPermission

# å¯¼å…¥æç¤ºè¯ç®¡ç†
from prompts.models import UserPrompt

# --- New Imports ---
from typing import TypedDict, Annotated, List
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_ollama import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models.tongyi import ChatTongyi
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages # Correct import for add_messages
from langgraph.checkpoint.sqlite import SqliteSaver # For sync operations in ChatHistoryAPIView
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver # Use Async version for async views
from langgraph.prebuilt import create_react_agent # For agent with tools
# from langgraph.checkpoint.memory import InMemorySaver # Remove InMemorySaver import if no longer globally needed
import os
import uuid # Import uuid module
# Knowledge base integration
from knowledge.langgraph_integration import KnowledgeRAGService, ConversationalRAGService, LangGraphKnowledgeIntegration
from knowledge.models import KnowledgeBase
import sqlite3 # Import sqlite3 module
from django.conf import settings
import logging # Import logging
from asgiref.sync import sync_to_async # For async operations in sync context
import json # For JSON serialization in streaming
import asyncio # For async operations

# Django streaming response
from django.http import StreamingHttpResponse

from mcp_tools.models import RemoteMCPConfig # To load remote MCP server configs
from langchain_mcp_adapters.client import MultiServerMCPClient # To connect to remote MCPs
from mcp_tools.persistent_client import mcp_session_manager # æŒä¹…åŒ–MCPä¼šè¯ç®¡ç†å™¨
# --- End New Imports ---

logger = logging.getLogger(__name__) # Initialize logger

# --- Helper Functions ---
def create_llm_instance(active_config, temperature=0.7):
    """
    æ ¹æ®é…ç½®åˆ›å»ºåˆé€‚çš„LLMå®ä¾‹ï¼Œæ”¯æŒå¤šç§ä¾›åº”å•†
    """
    model_identifier = active_config.name or "gpt-3.5-turbo"
    provider = active_config.provider
    
    if provider == 'anthropic':
        # Anthropic/Claude
        llm = ChatAnthropic(
            model=model_identifier,
            api_key=active_config.api_key,
            temperature=temperature
        )
        logger.info(f"Initialized ChatAnthropic with model: {model_identifier}")
    elif provider == 'openai':
        # OpenAI å®˜æ–¹
        llm = ChatOpenAI(
            model=model_identifier,
            temperature=temperature,
            api_key=active_config.api_key,
        )
        logger.info(f"Initialized ChatOpenAI with model: {model_identifier}")
    elif provider == 'ollama':
        # Ollama æœ¬åœ°éƒ¨ç½²
        llm = ChatOllama(
            model=model_identifier,
            base_url=active_config.api_url,
            temperature=temperature
        )
        logger.info(f"Initialized ChatOllama with model: {model_identifier}")
    elif provider == 'gemini':
        # Google Gemini
        llm = ChatGoogleGenerativeAI(
            model=model_identifier,
            google_api_key=active_config.api_key,
            temperature=temperature
        )
        logger.info(f"Initialized ChatGoogleGenerativeAI with model: {model_identifier}")
    elif provider == 'qwen':
        # Alibaba Qwen (Tongyi)
        llm = ChatTongyi(
            model=model_identifier,
            dashscope_api_key=active_config.api_key,
            temperature=temperature
        )
        logger.info(f"Initialized ChatTongyi with model: {model_identifier}")
    elif provider == 'openai_compatible':
        # OpenAI å…¼å®¹æœåŠ¡
        llm_kwargs = {
            "model": model_identifier,
            "temperature": temperature,
            "api_key": active_config.api_key,
            "base_url": active_config.api_url
        }
        
        llm = ChatOpenAI(**llm_kwargs)
        logger.info(f"Initialized OpenAI-compatible LLM with model: {model_identifier}")
    else:
        # é»˜è®¤ä½¿ç”¨OpenAI
        llm = ChatOpenAI(
            model=model_identifier,
            temperature=temperature,
            api_key=active_config.api_key,
        )
        logger.info(f"Initialized default ChatOpenAI with model: {model_identifier}")
    
    return llm

def create_sse_data(data_dict):
    """
    åˆ›å»ºSSEæ ¼å¼çš„æ•°æ®ï¼Œç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£ç¡®ç¼–ç 
    """
    json_str = json.dumps(data_dict, ensure_ascii=False)
    return f"data: {json_str}\n\n"

# --- AgentState Definition ---
class AgentState(TypedDict):
    messages: Annotated[List[AnyMessage], add_messages]
# --- End AgentState Definition ---

# --- Global Checkpointer ---
# This creates/uses a SQLite file in the project's BASE_DIR
# Ensure BASE_DIR is correctly defined in your settings.py
# settings.BASE_DIR should be a Path object or string.
# --- End Global Checkpointer ---
# Global checkpointer 'memory' is removed. It will be instantiated within the post method.

class LLMConfigViewSet(BaseModelViewSet):
    """
    LLMé…ç½®ç®¡ç†æ¥å£
    æä¾›å®Œæ•´çš„CRUDæ“ä½œ
    """
    queryset = LLMConfig.objects.all().order_by('-created_at')
    serializer_class = LLMConfigSerializer
    def perform_create(self, serializer):
        """æ‰§è¡Œåˆ›å»ºæ“ä½œ"""
        if serializer.validated_data.get('is_active', False):
            LLMConfig.objects.filter(is_active=True).update(is_active=False)
        serializer.save()

    def perform_update(self, serializer):
        """æ‰§è¡Œæ›´æ–°æ“ä½œ"""
        if serializer.validated_data.get('is_active', False):
            LLMConfig.objects.filter(is_active=True).exclude(pk=serializer.instance.pk).update(is_active=False)
        serializer.save()


def get_effective_system_prompt(user, prompt_id=None):
    """
    è·å–æœ‰æ•ˆçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
    ä¼˜å…ˆçº§ï¼šç”¨æˆ·æŒ‡å®šçš„æç¤ºè¯ > ç”¨æˆ·é»˜è®¤æç¤ºè¯ > å…¨å±€LLMé…ç½®çš„system_prompt

    Args:
        user: å½“å‰ç”¨æˆ·
        prompt_id: æŒ‡å®šçš„æç¤ºè¯IDï¼ˆå¯é€‰ï¼‰

    Returns:
        tuple: (prompt_content, prompt_source)
        prompt_content: æç¤ºè¯å†…å®¹
        prompt_source: æç¤ºè¯æ¥æº ('user_specified', 'user_default', 'global', 'none')
    """
    try:
        # 1. å¦‚æœæŒ‡å®šäº†æç¤ºè¯IDï¼Œä¼˜å…ˆä½¿ç”¨
        if prompt_id:
            try:
                user_prompt = UserPrompt.objects.get(
                    id=prompt_id,
                    user=user,
                    is_active=True
                )
                return user_prompt.content, 'user_specified'
            except UserPrompt.DoesNotExist:
                logger.warning(f"Specified prompt {prompt_id} not found for user {user.id}")

        # 2. å°è¯•è·å–ç”¨æˆ·çš„é»˜è®¤æç¤ºè¯
        default_prompt = UserPrompt.get_user_default_prompt(user)
        if default_prompt:
            return default_prompt.content, 'user_default'

        # 3. ä½¿ç”¨å…¨å±€LLMé…ç½®çš„system_prompt
        try:
            active_config = LLMConfig.objects.get(is_active=True)
            if active_config.system_prompt and active_config.system_prompt.strip():
                return active_config.system_prompt.strip(), 'global'
        except LLMConfig.DoesNotExist:
            logger.warning("No active LLM configuration found")

        # 4. æ²¡æœ‰ä»»ä½•æç¤ºè¯
        return None, 'none'

    except Exception as e:
        logger.error(f"Error getting effective system prompt: {e}")
        # é™çº§åˆ°å…¨å±€é…ç½®
        try:
            active_config = LLMConfig.objects.get(is_active=True)
            if active_config.system_prompt and active_config.system_prompt.strip():
                return active_config.system_prompt.strip(), 'global'
        except:
            pass
        return None, 'none'


async def get_effective_system_prompt_async(user, prompt_id=None):
    """
    è·å–æœ‰æ•ˆçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    ä¼˜å…ˆçº§ï¼šç”¨æˆ·æŒ‡å®šçš„æç¤ºè¯ > ç”¨æˆ·é»˜è®¤æç¤ºè¯ > å…¨å±€LLMé…ç½®çš„system_prompt

    Args:
        user: å½“å‰ç”¨æˆ·
        prompt_id: æŒ‡å®šçš„æç¤ºè¯IDï¼ˆå¯é€‰ï¼‰

    Returns:
        tuple: (prompt_content, prompt_source)
        prompt_content: æç¤ºè¯å†…å®¹
        prompt_source: æç¤ºè¯æ¥æº ('user_specified', 'user_default', 'global', 'none')
    """
    try:
        # 1. å¦‚æœæŒ‡å®šäº†æç¤ºè¯IDï¼Œä¼˜å…ˆä½¿ç”¨
        if prompt_id:
            try:
                user_prompt = await sync_to_async(UserPrompt.objects.get)(
                    id=prompt_id,
                    user=user,
                    is_active=True
                )
                return user_prompt.content, 'user_specified'
            except UserPrompt.DoesNotExist:
                logger.warning(f"Specified prompt {prompt_id} not found for user {user.id}")

        # 2. å°è¯•è·å–ç”¨æˆ·çš„é»˜è®¤æç¤ºè¯
        try:
            default_prompt = await sync_to_async(UserPrompt.objects.get)(
                user=user,
                is_default=True,
                is_active=True
            )
            if default_prompt:
                return default_prompt.content, 'user_default'
        except UserPrompt.DoesNotExist:
            pass

        # 3. ä½¿ç”¨å…¨å±€LLMé…ç½®çš„system_prompt
        try:
            active_config = await sync_to_async(LLMConfig.objects.get)(is_active=True)
            if active_config.system_prompt and active_config.system_prompt.strip():
                return active_config.system_prompt.strip(), 'global'
        except LLMConfig.DoesNotExist:
            logger.warning("No active LLM configuration found")

        # 4. æ²¡æœ‰ä»»ä½•æç¤ºè¯
        return None, 'none'

    except Exception as e:
        logger.error(f"Error getting effective system prompt: {e}")
        # é™çº§åˆ°å…¨å±€é…ç½®
        try:
            active_config = await sync_to_async(LLMConfig.objects.get)(is_active=True)
            if active_config.system_prompt and active_config.system_prompt.strip():
                return active_config.system_prompt.strip(), 'global'
        except:
            pass
        return None, 'none'


class ChatAPIView(APIView):
    """
    API endpoint for handling chat with the currently active LLM using LangGraph,
    with potential integration of remote MCP tools.
    æ”¯æŒé¡¹ç›®éš”ç¦»ï¼ŒèŠå¤©è®°å½•æŒ‰é¡¹ç›®åˆ†ç»„ã€‚
    """
    permission_classes = [HasModelPermission]

    def _check_project_permission(self, user, project_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®¿é—®æŒ‡å®šé¡¹ç›®çš„æƒé™"""
        try:
            project = Project.objects.get(id=project_id)
            # è¶…çº§ç”¨æˆ·å¯ä»¥è®¿é—®æ‰€æœ‰é¡¹ç›®
            if user.is_superuser:
                return project
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯é¡¹ç›®æˆå‘˜
            if ProjectMember.objects.filter(project=project, user=user).exists():
                return project
            return None
        except Project.DoesNotExist:
            return None

    async def dispatch(self, request, *args, **kwargs):
        """
        Handles incoming requests and ensures that the view is treated as async.
        """
        self.request = request
        self.args = args
        self.kwargs = kwargs
        # Ensure request object is initialized for DRF's typical expectations
        # This might involve more complex handling if request.user is accessed early by sync code
        # For now, we assume standard DRF request processing can be wrapped.
        request = await sync_to_async(self.initialize_request)(request, *args, **kwargs)
        self.request = request
        self.headers = await sync_to_async(lambda: self.default_response_headers)()

        try:
            await sync_to_async(self.initial)(request, *args, **kwargs)

            if request.method.lower() in self.http_method_names:
                handler = getattr(self, request.method.lower(), self.http_method_not_allowed)
            else:
                handler = self.http_method_not_allowed

            response = await handler(request, *args, **kwargs)

        except Exception as exc:
            response = await sync_to_async(self.handle_exception)(exc)

        self.response = await sync_to_async(self.finalize_response)(request, response, *args, **kwargs)
        return self.response

    async def post(self, request, *args, **kwargs):
        logger.info(f"ChatAPIView: Received POST request from user {request.user.id}")
        user_message_content = request.data.get('message')
        session_id = request.data.get('session_id')
        project_id = request.data.get('project_id')
        image_base64 = request.data.get('image')  # å›¾ç‰‡base64ç¼–ç ï¼ˆä¸å«å‰ç¼€ï¼‰

        # çŸ¥è¯†åº“ç›¸å…³å‚æ•°
        knowledge_base_id = request.data.get('knowledge_base_id')
        use_knowledge_base = request.data.get('use_knowledge_base', True)  # é»˜è®¤å¯ç”¨çŸ¥è¯†åº“
        similarity_threshold = request.data.get('similarity_threshold', 0.7)
        top_k = request.data.get('top_k', 5)

        # æç¤ºè¯ç›¸å…³å‚æ•°
        prompt_id = request.data.get('prompt_id')  # ç”¨æˆ·æŒ‡å®šçš„æç¤ºè¯ID

        # éªŒè¯é¡¹ç›®IDæ˜¯å¦æä¾›
        if not project_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "project_id is required.", "data": {},
                "errors": {"project_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥é¡¹ç›®æƒé™
        project = await sync_to_async(self._check_project_permission)(request.user, project_id)
        if not project:
            return Response({
                "status": "error", "code": status.HTTP_403_FORBIDDEN,
                "message": "You don't have permission to access this project or project doesn't exist.", "data": {},
                "errors": {"project_id": ["Permission denied or project not found."]}
            }, status=status.HTTP_403_FORBIDDEN)

        is_new_session = False
        if not session_id:
            session_id = uuid.uuid4().hex
            is_new_session = True
            logger.info(f"ChatAPIView: Generated new session_id: {session_id}")

        if not user_message_content:
            logger.warning("ChatAPIView: Message content is required but not provided.")
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "Message content is required.", "data": {},
                "errors": {"message": ["This field may not be blank."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        try:
            # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œç«‹å³åˆ›å»ºChatSessionå¯¹è±¡
            if is_new_session:
                try:
                    await sync_to_async(ChatSession.objects.create)(
                        user=request.user,
                        session_id=session_id,
                        project=project,
                        title=f"æ–°å¯¹è¯ - {user_message_content[:30]}" # ä½¿ç”¨æ¶ˆæ¯å†…å®¹ä½œä¸ºä¸´æ—¶æ ‡é¢˜
                    )
                    logger.info(f"ChatAPIView: Created new ChatSession entry for session_id: {session_id}")
                except Exception as e:
                    logger.error(f"ChatAPIView: Failed to create ChatSession entry: {e}", exc_info=True)

            active_config = await sync_to_async(LLMConfig.objects.get)(is_active=True)
            logger.info(f"ChatAPIView: Using active LLMConfig: {active_config.name}")
        except LLMConfig.DoesNotExist:
            logger.error("ChatAPIView: No active LLM configuration found.")
            return Response({
                "status": "error", "code": status.HTTP_503_SERVICE_UNAVAILABLE,
                "message": "No active LLM configuration found. Please configure and activate an LLM.", "data": {},
                "errors": {"llm_config": ["No active LLM configuration available."]}
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        except LLMConfig.MultipleObjectsReturned:
            logger.error("ChatAPIView: Multiple active LLM configurations found.")
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": "Multiple active LLM configurations found. Ensure only one is active.", "data": {},
                "errors": {"llm_config": ["Multiple active LLM configurations found."]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

        # éªŒè¯å›¾ç‰‡è¾“å…¥æ˜¯å¦æ”¯æŒ
        if image_base64 and not active_config.supports_vision:
            logger.warning(f"ChatAPIView: Image input rejected - model {active_config.name} does not support vision")
            return Response({
                "status": "error",
                "code": status.HTTP_400_BAD_REQUEST,
                "message": f"å½“å‰æ¨¡å‹ {active_config.name} ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼Œè¯·åˆ‡æ¢åˆ°æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4Vã€Claude 3ã€Gemini Vision æˆ– Qwen-VLï¼‰",
                "data": {},
                "errors": {"image": ["Current model does not support image input. Please switch to a vision-capable model."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        try:
            # ä½¿ç”¨æ–°çš„LLMå·¥å‚å‡½æ•°ï¼Œæ”¯æŒå¤šä¾›åº”å•†
            llm = create_llm_instance(active_config, temperature=0.7)
            logger.info(f"ChatAPIView: Initialized LLM with provider auto-detection")

            db_path = os.path.join(str(settings.BASE_DIR), "chat_history.sqlite")
            async with AsyncSqliteSaver.from_conn_string(db_path) as actual_memory_checkpointer: # Use async with and AsyncSqliteSaver
                # Load remote MCP tools
                logger.info("ChatAPIView: Attempting to load remote MCP tools.")
                mcp_tools_list = []
                try:
                    active_remote_mcp_configs_qs = RemoteMCPConfig.objects.filter(is_active=True)
                    active_remote_mcp_configs = await sync_to_async(list)(active_remote_mcp_configs_qs)

                    if active_remote_mcp_configs:
                        client_mcp_config = {}
                        for r_config in active_remote_mcp_configs:
                            config_key = r_config.name or f"remote_config_{r_config.id}"
                            client_mcp_config[config_key] = {
                                "url": r_config.url,
                                "transport": (r_config.transport or "streamable_http").replace('-', '_'),
                            }
                            if r_config.headers and isinstance(r_config.headers, dict) and r_config.headers:
                                client_mcp_config[config_key]["headers"] = r_config.headers

                        if client_mcp_config:
                            logger.info(f"ChatAPIView: Initializing persistent MCP client with config: {client_mcp_config}")
                            # ä½¿ç”¨æŒä¹…åŒ–MCPä¼šè¯ç®¡ç†å™¨ï¼Œä¼ é€’ç”¨æˆ·ã€é¡¹ç›®å’Œä¼šè¯ä¿¡æ¯ä»¥æ”¯æŒè·¨å¯¹è¯è½®æ¬¡çš„çŠ¶æ€ä¿æŒ
                            mcp_tools_list = await mcp_session_manager.get_tools_for_config(
                                client_mcp_config,
                                user_id=str(request.user.id),
                                project_id=str(project_id),
                                session_id=session_id  # ä¼ é€’session_idä»¥å¯ç”¨ä¼šè¯çº§åˆ«çš„å·¥å…·ç¼“å­˜
                            )
                            logger.info(f"ChatAPIView: Successfully loaded {len(mcp_tools_list)} persistent tools from remote MCP servers: {[tool.name for tool in mcp_tools_list if hasattr(tool, 'name')]}")
                        else:
                            logger.info("ChatAPIView: No active remote MCP configurations to build client config.")
                    else:
                        logger.info("ChatAPIView: No active RemoteMCPConfig found.")
                except Exception as e: # Catches errors from mcp_client.get_tools() like HTTP 429
                    logger.error(f"ChatAPIView: Error loading remote MCP tools: {e}", exc_info=True)
                    # mcp_tools_list remains empty, will fallback to basic chatbot

                # Prepare LangGraph runnable
                runnable_to_invoke = None
                is_agent_with_tools = False

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºAgentï¼ˆæœ‰MCPå·¥å…·ï¼‰
                if mcp_tools_list:
                    logger.info(f"ChatAPIView: Attempting to create agent with {len(mcp_tools_list)} remote tools.")
                    try:
                        # å¦‚æœåŒæ—¶æœ‰çŸ¥è¯†åº“å’ŒMCPå·¥å…·ï¼Œåˆ›å»ºçŸ¥è¯†åº“å¢å¼ºçš„Agent
                        if knowledge_base_id and use_knowledge_base:
                            logger.info(f"ChatAPIView: Creating knowledge-enhanced agent with {len(mcp_tools_list)} tools and knowledge base {knowledge_base_id}")

                            # åˆ›å»ºçŸ¥è¯†åº“å·¥å…·
                            from knowledge.langgraph_integration import create_knowledge_tool
                            knowledge_tool = create_knowledge_tool(
                                knowledge_base_id=knowledge_base_id,
                                user=request.user,
                                similarity_threshold=similarity_threshold,
                                top_k=top_k
                            )

                            # å°†çŸ¥è¯†åº“å·¥å…·æ·»åŠ åˆ°MCPå·¥å…·åˆ—è¡¨
                            enhanced_tools = mcp_tools_list + [knowledge_tool]
                            agent_executor = create_react_agent(llm, enhanced_tools, checkpointer=actual_memory_checkpointer)
                            runnable_to_invoke = agent_executor
                            is_agent_with_tools = True
                            logger.info(f"ChatAPIView: Knowledge-enhanced agent created with {len(enhanced_tools)} tools (including knowledge base)")
                        else:
                            # åªæœ‰MCPå·¥å…·ï¼Œåˆ›å»ºæ™®é€šAgent
                            agent_executor = create_react_agent(llm, mcp_tools_list, checkpointer=actual_memory_checkpointer)
                            runnable_to_invoke = agent_executor
                            is_agent_with_tools = True
                            logger.info("ChatAPIView: Agent with remote tools created with checkpointer.")
                    except Exception as e:
                        logger.error(f"ChatAPIView: Failed to create agent with remote tools: {e}. Falling back to knowledge-enhanced chatbot.", exc_info=True)

                if not runnable_to_invoke:
                    logger.info("ChatAPIView: No remote tools or agent creation failed. Using knowledge-enhanced chatbot.")
                    is_agent_with_tools = False # Ensure flag is false for basic chatbot

                    def knowledge_enhanced_chatbot_node(state: AgentState):
                        """çŸ¥è¯†åº“å¢å¼ºçš„èŠå¤©æœºå™¨äººèŠ‚ç‚¹"""
                        try:
                            # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
                            user_messages = [msg for msg in state['messages']
                                           if isinstance(msg, HumanMessage)]

                            if not user_messages:
                                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œç›´æ¥è°ƒç”¨LLM
                                invoked_response = llm.invoke(state['messages'])
                                return {"messages": [invoked_response]}

                            latest_user_message = user_messages[-1].content

                            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨çŸ¥è¯†åº“
                            should_use_kb = use_knowledge_base and knowledge_base_id

                            if should_use_kb:
                                logger.info(f"ChatAPIView: Using knowledge base {knowledge_base_id} for query")

                                # ä½¿ç”¨çŸ¥è¯†åº“RAGæœåŠ¡
                                from knowledge.langgraph_integration import ConversationalRAGService
                                rag_service = ConversationalRAGService(llm)

                                # æ‰§è¡ŒRAGæŸ¥è¯¢
                                rag_result = rag_service.query(
                                    question=latest_user_message,
                                    knowledge_base_id=knowledge_base_id,
                                    user=request.user,
                                    project_id=project_id,
                                    thread_id=thread_id,
                                    use_knowledge_base=True,
                                    similarity_threshold=similarity_threshold,
                                    top_k=top_k
                                )

                                # è¿”å›RAGç»“æœä¸­çš„æ¶ˆæ¯
                                rag_messages = rag_result.get("messages", [])
                                if rag_messages:
                                    logger.info(f"ChatAPIView: RAG returned {len(rag_messages)} messages")
                                    return {"messages": rag_messages}
                                else:
                                    logger.warning("ChatAPIView: RAG returned no messages, falling back to basic chat")

                            # é™çº§åˆ°åŸºç¡€å¯¹è¯
                            logger.info("ChatAPIView: Using basic chat without knowledge base")
                            invoked_response = llm.invoke(state['messages'])
                            return {"messages": [invoked_response]}

                        except Exception as e:
                            logger.error(f"ChatAPIView: Error in knowledge-enhanced chatbot: {e}")
                            # é™çº§åˆ°åŸºç¡€å¯¹è¯
                            invoked_response = llm.invoke(state['messages'])
                            return {"messages": [invoked_response]}

                    graph_builder = StateGraph(AgentState)
                    graph_builder.add_node("chatbot", knowledge_enhanced_chatbot_node)
                    graph_builder.set_entry_point("chatbot")
                    graph_builder.add_edge("chatbot", END)
                    runnable_to_invoke = graph_builder.compile(checkpointer=actual_memory_checkpointer) # Use actual checkpointer instance
                    logger.info("ChatAPIView: Knowledge-enhanced chatbot graph compiled.")

                # Determine thread_id - åŒ…å«é¡¹ç›®IDä»¥å®ç°é¡¹ç›®éš”ç¦»
                thread_id_parts = [str(request.user.id), str(project_id)]
                if session_id:
                    thread_id_parts.append(str(session_id))
                thread_id = "_".join(thread_id_parts)
                logger.info(f"ChatAPIView: Using thread_id: {thread_id} for project: {project.name}")

                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯
                messages_list = []

                # è·å–æœ‰æ•ˆçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨æˆ·æç¤ºè¯ä¼˜å…ˆï¼‰
                effective_prompt, prompt_source = await get_effective_system_prompt_async(request.user, prompt_id)

                # æ£€æŸ¥å½“å‰ä¼šè¯æ˜¯å¦å·²ç»æœ‰ç³»ç»Ÿæç¤ºè¯
                should_add_system_prompt = False
                if effective_prompt:
                    try:
                        # å°è¯•è·å–å½“å‰ä¼šè¯çš„å†å²æ¶ˆæ¯
                        with SqliteSaver.from_conn_string(db_path) as memory:
                            checkpoint_generator = memory.list(config={"configurable": {"thread_id": thread_id}})
                            checkpoint_tuples_list = list(checkpoint_generator)

                            if checkpoint_tuples_list:
                                # æ£€æŸ¥æœ€æ–°checkpointä¸­æ˜¯å¦å·²æœ‰ç³»ç»Ÿæç¤ºè¯
                                latest_checkpoint = checkpoint_tuples_list[0].checkpoint
                                if (latest_checkpoint and 'channel_values' in latest_checkpoint
                                    and 'messages' in latest_checkpoint['channel_values']):
                                    existing_messages = latest_checkpoint['channel_values']['messages']
                                    # æ£€æŸ¥ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯ç³»ç»Ÿæ¶ˆæ¯
                                    if not existing_messages or not isinstance(existing_messages[0], SystemMessage):
                                        should_add_system_prompt = True
                                else:
                                    should_add_system_prompt = True
                            else:
                                # æ–°ä¼šè¯ï¼Œéœ€è¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯
                                should_add_system_prompt = True
                    except Exception as e:
                        logger.warning(f"ChatAPIView: Error checking existing messages: {e}")
                        should_add_system_prompt = True

                if should_add_system_prompt and effective_prompt:
                    messages_list.append(SystemMessage(content=effective_prompt))
                    logger.info(f"ChatAPIView: Added {prompt_source} system prompt: {effective_prompt[:100]}...")

                # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
                if image_base64:
                    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œåˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
                    human_message_content = [
                        {"type": "text", "text": user_message_content},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                        }
                    ]
                    messages_list.append(HumanMessage(content=human_message_content))
                    logger.info(f"ChatAPIView: Added multimodal message with image")
                else:
                    # çº¯æ–‡æœ¬æ¶ˆæ¯
                    messages_list.append(HumanMessage(content=user_message_content))
                input_messages = {"messages": messages_list}

                invoke_config = {
                    "configurable": {"thread_id": thread_id},
                    "recursion_limit": 100  # å¢åŠ é€’å½’é™åˆ¶ï¼Œæ”¯æŒç”Ÿæˆæ›´å¤šæµ‹è¯•ç”¨ä¾‹
                }
                logger.info(f"ChatAPIView: Set recursion_limit to 100 for thread_id: {thread_id}")
                # Checkpointer is already configured in both agent and basic chatbot

                final_state = await runnable_to_invoke.ainvoke(
                    input_messages,
                    config=invoke_config
                )

                ai_response_content = "No valid AI response found."
                conversation_flow = []  # å­˜å‚¨å®Œæ•´çš„å¯¹è¯æµç¨‹

                if final_state and final_state.get('messages'):
                    # å¤„ç†æ‰€æœ‰æ¶ˆæ¯ï¼Œæå–å¯¹è¯æµç¨‹
                    messages = final_state['messages']
                    logger.info(f"ChatAPIView: Processing {len(messages)} messages in final state")

                    # æ‰¾åˆ°æœ¬æ¬¡å¯¹è¯çš„èµ·å§‹ä½ç½®ï¼ˆç”¨æˆ·åˆšå‘é€çš„æ¶ˆæ¯ï¼‰
                    user_message_index = -1
                    for i, msg in enumerate(messages):
                        if isinstance(msg, HumanMessage) and msg.content == user_message_content:
                            user_message_index = i
                            break

                    # å¦‚æœæ‰¾åˆ°äº†ç”¨æˆ·æ¶ˆæ¯ï¼Œæå–ä»è¯¥æ¶ˆæ¯å¼€å§‹çš„æ‰€æœ‰åç»­æ¶ˆæ¯
                    if user_message_index >= 0:
                        current_conversation = messages[user_message_index:]

                        for i, msg in enumerate(current_conversation):
                            msg_type = "unknown"
                            content = ""

                            if isinstance(msg, SystemMessage):
                                msg_type = "system"
                                content = msg.content if hasattr(msg, 'content') else str(msg)
                            elif isinstance(msg, HumanMessage):
                                msg_type = "human"
                                content = msg.content if hasattr(msg, 'content') else str(msg)
                            elif isinstance(msg, AIMessage):
                                msg_type = "ai"
                                content = msg.content if hasattr(msg, 'content') else str(msg)

                                # è·³è¿‡ç©ºçš„AIæ¶ˆæ¯ï¼ˆå·¥å…·è°ƒç”¨å‰çš„ä¸­é—´çŠ¶æ€ï¼‰
                                if not content or content.strip() == "":
                                    logger.debug(f"ChatAPIView: Skipping empty AI message at index {i}")
                                    continue

                            elif isinstance(msg, ToolMessage):
                                msg_type = "tool"
                                content = msg.content if hasattr(msg, 'content') else str(msg)
                            else:
                                # å¤„ç†å…¶ä»–ç±»å‹çš„æ¶ˆæ¯ï¼Œå¯èƒ½æ˜¯å·¥å…·è°ƒç”¨ç»“æœ
                                content = msg.content if hasattr(msg, 'content') else str(msg)
                                # å¦‚æœå†…å®¹çœ‹èµ·æ¥åƒJSONï¼Œå¯èƒ½æ˜¯å·¥å…·è¿”å›
                                if content.strip().startswith('[') or content.strip().startswith('{'):
                                    msg_type = "tool"
                                else:
                                    msg_type = "unknown"

                            # åªæ·»åŠ æœ‰å†…å®¹çš„æ¶ˆæ¯
                            if content and content.strip():
                                conversation_flow.append({
                                    "type": msg_type,
                                    "content": content
                                })

                                # è®°å½•æœ€åä¸€æ¡AIæ¶ˆæ¯ä½œä¸ºä¸»è¦å›å¤
                                if msg_type == "ai":
                                    ai_response_content = content

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯ï¼Œä½¿ç”¨æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºå›å¤
                    if user_message_index == -1 and messages:
                        last_message = messages[-1]
                        if hasattr(last_message, 'content'):
                            ai_response_content = last_message.content

                logger.info(f"ChatAPIView: Successfully processed message for thread_id: {thread_id}. AI response: {ai_response_content[:100]}...")
                logger.info(f"ChatAPIView: Conversation flow contains {len(conversation_flow)} messages")

                return Response({
                    "status": "success", "code": status.HTTP_200_OK,
                    "message": "Message processed successfully.",
                    "data": {
                        "user_message": user_message_content,
                        "llm_response": ai_response_content,
                        "conversation_flow": conversation_flow,  # æ–°å¢ï¼šå®Œæ•´çš„å¯¹è¯æµç¨‹
                        "active_llm": active_config.name,
                        "thread_id": thread_id,
                        "session_id": session_id,
                        "project_id": project_id,
                        "project_name": project.name,
                        # çŸ¥è¯†åº“ç›¸å…³ä¿¡æ¯
                        "knowledge_base_id": knowledge_base_id,
                        "use_knowledge_base": use_knowledge_base,
                        "knowledge_base_used": bool(knowledge_base_id and use_knowledge_base)
                    }
                }, status=status.HTTP_200_OK)

        except Exception as e: # This outer try-except catches errors from the 'with SqliteSaver' block or LLM init
            logger.error(f"ChatAPIView: Error interacting with LLM or LangGraph: {e}", exc_info=True)
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"Error interacting with LLM or LangGraph: {str(e)}", "data": {},
                "errors": {"llm_interaction": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class ChatHistoryAPIView(APIView):
    """
    API endpoint for retrieving chat history for a given session_id.
    æ”¯æŒé¡¹ç›®éš”ç¦»ï¼Œåªèƒ½è·å–æŒ‡å®šé¡¹ç›®çš„èŠå¤©è®°å½•ã€‚
    """
    permission_classes = [permissions.IsAuthenticated]

    def _check_project_permission(self, user, project_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®¿é—®æŒ‡å®šé¡¹ç›®çš„æƒé™"""
        try:
            project = Project.objects.get(id=project_id)
            # è¶…çº§ç”¨æˆ·å¯ä»¥è®¿é—®æ‰€æœ‰é¡¹ç›®
            if user.is_superuser:
                return project
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯é¡¹ç›®æˆå‘˜
            if ProjectMember.objects.filter(project=project, user=user).exists():
                return project
            return None
        except Project.DoesNotExist:
            return None

    def get(self, request, *args, **kwargs):
        session_id = request.query_params.get('session_id')
        project_id = request.query_params.get('project_id')

        if not session_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "session_id query parameter is required.", "data": {},
                "errors": {"session_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        if not project_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "project_id query parameter is required.", "data": {},
                "errors": {"project_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥é¡¹ç›®æƒé™
        project = self._check_project_permission(request.user, project_id)
        if not project:
            return Response({
                "status": "error", "code": status.HTTP_403_FORBIDDEN,
                "message": "You don't have permission to access this project or project doesn't exist.", "data": {},
                "errors": {"project_id": ["Permission denied or project not found."]}
            }, status=status.HTTP_403_FORBIDDEN)

        thread_id_parts = [str(request.user.id), str(project_id), str(session_id)]
        thread_id = "_".join(thread_id_parts)

        db_path = os.path.join(str(settings.BASE_DIR), "chat_history.sqlite")
        history_messages = []

        try:
            # é¦–å…ˆå°è¯•ç›´æ¥æŸ¥è¯¢æ•°æ®åº“ä»¥æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„thread_idè®°å½•
            cursor.execute("SELECT COUNT(*) FROM checkpoints WHERE thread_id = ?", (thread_id,))
            checkpoint_count = cursor.fetchone()[0]
            logger.info(f"ChatHistoryAPIView: Found {checkpoint_count} checkpoints in database for thread_id: {thread_id}")

            if checkpoint_count == 0:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®°å½•ï¼Œæ£€æŸ¥æ‰€æœ‰çš„thread_id
                cursor.execute("SELECT DISTINCT thread_id FROM checkpoints LIMIT 10")
                all_threads = cursor.fetchall()
                logger.info(f"ChatHistoryAPIView: Available thread_ids in database: {[t[0] for t in all_threads]}")

            conn.close()

            # ä½¿ç”¨SqliteSaverè¯»å–æ•°æ®
            with SqliteSaver.from_conn_string(db_path) as memory:
                # Fetch all checkpoints for the thread
                # The list method returns CheckpointTuple, we need the 'checkpoint' attribute
                checkpoint_generator = memory.list(config={"configurable": {"thread_id": thread_id}})
                checkpoint_tuples_list = list(checkpoint_generator) # Convert generator to list

                logger.info(f"ChatHistoryAPIView: SqliteSaver found {len(checkpoint_tuples_list)} checkpoints for thread_id: {thread_id}")

                if checkpoint_tuples_list: # Check if the list is not empty
                    # æ„å»ºæ¶ˆæ¯åˆ°æ—¶é—´æˆ³çš„æ˜ å°„
                    # éå†æ‰€æœ‰checkpointsï¼Œä¸ºæ¯æ¡æ–°æ¶ˆæ¯åˆ†é…å¯¹åº”checkpointçš„æ—¶é—´æˆ³
                    message_timestamps = {}
                    processed_message_count = 0

                    # æŒ‰æ—¶é—´é¡ºåºå¤„ç†checkpointsï¼ˆä»æ—§åˆ°æ–°ï¼‰
                    for checkpoint_tuple in reversed(checkpoint_tuples_list):
                        if checkpoint_tuple and hasattr(checkpoint_tuple, 'checkpoint'):
                            checkpoint_data = checkpoint_tuple.checkpoint
                            if checkpoint_data and 'channel_values' in checkpoint_data and 'messages' in checkpoint_data['channel_values']:
                                messages = checkpoint_data['channel_values']['messages']
                                current_message_count = len(messages)

                                # å¦‚æœè¿™ä¸ªcheckpointæœ‰æ–°æ¶ˆæ¯ï¼Œä¸ºæ–°æ¶ˆæ¯åˆ†é…æ—¶é—´æˆ³
                                if current_message_count > processed_message_count:
                                    checkpoint_timestamp = checkpoint_data.get('ts')
                                    if checkpoint_timestamp:
                                        # ä¸ºæ–°å¢çš„æ¶ˆæ¯åˆ†é…æ—¶é—´æˆ³
                                        for i in range(processed_message_count, current_message_count):
                                            message_timestamps[i] = checkpoint_timestamp
                                    processed_message_count = current_message_count

                    # è·å–æœ€æ–°checkpointçš„æ¶ˆæ¯åˆ—è¡¨
                    latest_checkpoint_tuple = checkpoint_tuples_list[0]
                    if latest_checkpoint_tuple and hasattr(latest_checkpoint_tuple, 'checkpoint'):
                        checkpoint_data = latest_checkpoint_tuple.checkpoint
                        logger.info(f"ChatHistoryAPIView: Processing checkpoint with keys: {list(checkpoint_data.keys()) if checkpoint_data else 'None'}")

                        if checkpoint_data and 'channel_values' in checkpoint_data and 'messages' in checkpoint_data['channel_values']:
                            messages = checkpoint_data['channel_values']['messages']
                            logger.info(f"ChatHistoryAPIView: Found {len(messages)} messages in latest checkpoint")

                            for i, msg in enumerate(messages):
                                msg_type = "unknown"
                                content = ""

                                if isinstance(msg, SystemMessage):
                                    msg_type = "system"
                                    content = msg.content if hasattr(msg, 'content') else str(msg)
                                elif isinstance(msg, HumanMessage):
                                    msg_type = "human"
                                    raw_content = msg.content if hasattr(msg, 'content') else str(msg)
                                    # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆåŒ…å«å›¾ç‰‡çš„åˆ—è¡¨æ ¼å¼ï¼‰
                                    image_data = None  # ç”¨äºå­˜å‚¨å›¾ç‰‡æ•°æ®
                                    if isinstance(raw_content, list):
                                        # æå–æ–‡æœ¬éƒ¨åˆ†
                                        text_parts = []
                                        for item in raw_content:
                                            if isinstance(item, dict):
                                                if item.get("type") == "text":
                                                    text_parts.append(item.get("text", ""))
                                                elif item.get("type") == "image_url":
                                                    # æå–å›¾ç‰‡URLä¸­çš„Base64æ•°æ®
                                                    image_url = item.get("image_url", {})
                                                    if isinstance(image_url, dict):
                                                        url = image_url.get("url", "")
                                                        # urlæ ¼å¼: data:image/jpeg;base64,xxx
                                                        if url and url.startswith("data:image/"):
                                                            image_data = url  # ä¿å­˜å®Œæ•´çš„Data URL
                                        content = " ".join(text_parts) if text_parts else "[åŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯]"
                                    else:
                                        content = raw_content
                                elif isinstance(msg, AIMessage):
                                    msg_type = "ai"
                                    raw_content = msg.content if hasattr(msg, 'content') else str(msg)
                                    # AIæ¶ˆæ¯é€šå¸¸ä¸æ˜¯å¤šæ¨¡æ€ï¼Œä½†ä¸ºäº†å®‰å…¨ä¹Ÿæ£€æŸ¥ä¸€ä¸‹
                                    if isinstance(raw_content, list):
                                        text_parts = [item.get("text", "") for item in raw_content if isinstance(item, dict) and item.get("type") == "text"]
                                        content = " ".join(text_parts) if text_parts else ""
                                    else:
                                        content = raw_content

                                    # è·³è¿‡ç©ºçš„AIæ¶ˆæ¯ï¼ˆå·¥å…·è°ƒç”¨å‰çš„ä¸­é—´çŠ¶æ€ï¼‰
                                    if not content or (isinstance(content, str) and content.strip() == ""):
                                        logger.debug(f"ChatHistoryAPIView: Skipping empty AI message at index {i}")
                                        continue
                                    
                                    # æå–additional_kwargsä¸­çš„agentä¿¡æ¯
                                    agent_info = None
                                    agent_type = None
                                    if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                        agent_info = msg.additional_kwargs.get('agent')
                                        agent_type = msg.additional_kwargs.get('agent_type')
                                        logger.debug(f"ChatHistoryAPIView: AI message has agent info: {agent_info}, type: {agent_type}")

                                elif isinstance(msg, ToolMessage):
                                    msg_type = "tool"
                                    content = msg.content if hasattr(msg, 'content') else str(msg)
                                else:
                                    # å¤„ç†å…¶ä»–ç±»å‹çš„æ¶ˆæ¯ï¼Œå¯èƒ½æ˜¯å·¥å…·è°ƒç”¨ç»“æœ
                                    content = msg.content if hasattr(msg, 'content') else str(msg)
                                    # å¦‚æœå†…å®¹çœ‹èµ·æ¥åƒJSONï¼Œå¯èƒ½æ˜¯å·¥å…·è¿”å›
                                    if content.strip().startswith('[') or content.strip().startswith('{'):
                                        msg_type = "tool"
                                    else:
                                        msg_type = "unknown"

                                logger.debug(f"ChatHistoryAPIView: Message {i}: type={msg_type}, content={str(content)[:50]}...")

                                # åªæ·»åŠ æœ‰å†…å®¹çš„æ¶ˆæ¯
                                if content and (not isinstance(content, str) or content.strip()):
                                    message_data = {
                                        "type": msg_type,
                                        "content": content,
                                    }
                                    # å¦‚æœæ¶ˆæ¯åŒ…å«å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡æ•°æ®
                                    if msg_type == "human" and 'image_data' in locals() and image_data:
                                        message_data["image"] = image_data
                                    # å¦‚æœAIæ¶ˆæ¯åŒ…å«agentä¿¡æ¯ï¼Œæ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
                                    if msg_type == "ai" and 'agent_info' in locals() and agent_info:
                                        message_data["agent"] = agent_info
                                        if 'agent_type' in locals() and agent_type:
                                            message_data["agent_type"] = agent_type
                                        # ğŸ¨ æ£€æŸ¥æ˜¯å¦æ˜¯æ€è€ƒè¿‡ç¨‹æ¶ˆæ¯
                                        if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                            if msg.additional_kwargs.get('is_thinking_process'):
                                                message_data["is_thinking_process"] = True
                                    # æ·»åŠ å¯¹åº”çš„æ—¶é—´æˆ³
                                    if i in message_timestamps:
                                        timestamp_str = message_timestamps[i]
                                        try:
                                            # è§£æISOæ—¶é—´æˆ³å¹¶è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
                                            from datetime import datetime
                                            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                                            # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
                                            local_dt = dt.astimezone()
                                            # æ ¼å¼åŒ–ä¸ºæœ¬åœ°æ—¶é—´å­—ç¬¦ä¸²
                                            message_data["timestamp"] = local_dt.strftime("%Y-%m-%d %H:%M:%S")
                                        except Exception as e:
                                            # å¦‚æœè§£æå¤±è´¥ï¼Œåªè¿”å›åŸå§‹å­—ç¬¦ä¸²
                                            logger.warning(f"ChatHistoryAPIView: Failed to parse timestamp {timestamp_str}: {e}")
                                            message_data["timestamp"] = timestamp_str

                                    history_messages.append(message_data)
                        else:
                            logger.warning(f"ChatHistoryAPIView: No messages found in checkpoint data structure")
                    else:
                        logger.warning(f"ChatHistoryAPIView: Invalid checkpoint tuple structure")
                else:
                    logger.info(f"ChatHistoryAPIView: No checkpoints found for thread_id: {thread_id}")
            # By processing only the latest checkpoint, we get the final state of messages, avoiding duplicates.

            return Response({
                "status": "success", "code": status.HTTP_200_OK,
                "message": "Chat history retrieved successfully.",
                "data": {
                    "thread_id": thread_id,
                    "session_id": session_id,
                    "project_id": project_id,
                    "project_name": project.name,
                    "history": history_messages
                }
            }, status=status.HTTP_200_OK)

        except FileNotFoundError:
             return Response({
                "status": "success", "code": status.HTTP_200_OK, # Or 404 if preferred for no history file
                "message": "No chat history found for this session (history file does not exist).",
                "data": {
                    "thread_id": thread_id,
                    "session_id": session_id,
                    "history": []
                }
            }, status=status.HTTP_200_OK)
        except Exception as e:
            # import logging
            # logging.exception(f"Error retrieving chat history for thread_id {thread_id}")
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"Error retrieving chat history: {str(e)}", "data": {},
                "errors": {"history_retrieval": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def delete(self, request, *args, **kwargs):
        session_id = request.query_params.get('session_id')
        project_id = request.query_params.get('project_id')

        if not session_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "session_id query parameter is required.", "data": {},
                "errors": {"session_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        if not project_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "project_id query parameter is required.", "data": {},
                "errors": {"project_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥é¡¹ç›®æƒé™
        project = self._check_project_permission(request.user, project_id)
        if not project:
            return Response({
                "status": "error", "code": status.HTTP_403_FORBIDDEN,
                "message": "You don't have permission to access this project or project doesn't exist.", "data": {},
                "errors": {"project_id": ["Permission denied or project not found."]}
            }, status=status.HTTP_403_FORBIDDEN)

        thread_id_parts = [str(request.user.id), str(project_id), str(session_id)]
        thread_id = "_".join(thread_id_parts)

        db_path = os.path.join(str(settings.BASE_DIR), "chat_history.sqlite")

        if not os.path.exists(db_path):
            return Response({
                "status": "success", # Or "error" with 404 if preferred
                "code": status.HTTP_200_OK, # Or 404
                "message": "No chat history found to delete (history file does not exist).",
                "data": {"thread_id": thread_id, "session_id": session_id, "deleted_count": 0}
            }, status=status.HTTP_200_OK)

        conn = None
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # It's good practice to check how many rows were affected.
            cursor.execute("DELETE FROM checkpoints WHERE thread_id = ?", (thread_id,))
            deleted_count = cursor.rowcount # Get the number of rows deleted

            conn.commit()

            if deleted_count > 0:
                message = f"Successfully deleted chat history for session_id: {session_id} (Thread ID: {thread_id}). {deleted_count} records removed."
            else:
                message = f"No chat history found for session_id: {session_id} (Thread ID: {thread_id}) to delete."

            return Response({
                "status": "success", "code": status.HTTP_200_OK,
                "message": message,
                "data": {"thread_id": thread_id, "session_id": session_id, "deleted_count": deleted_count}
            }, status=status.HTTP_200_OK)

        except sqlite3.Error as e:
            # import logging
            # logging.exception(f"SQLite error deleting chat history for thread_id {thread_id}: {e}")
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"Database error while deleting chat history: {str(e)}", "data": {},
                "errors": {"database_error": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except Exception as e:
            # import logging
            # logging.exception(f"Unexpected error deleting chat history for thread_id {thread_id}: {e}")
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"An unexpected error occurred: {str(e)}", "data": {},
                "errors": {"unexpected_error": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        finally:
            if conn:
                conn.close()


class ChatBatchDeleteAPIView(APIView):
    """
    API endpoint for batch deleting chat sessions.
    æ‰¹é‡åˆ é™¤èŠå¤©ä¼šè¯çš„APIç«¯ç‚¹ã€‚
    """
    permission_classes = [permissions.IsAuthenticated]

    def _check_project_permission(self, user, project_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®¿é—®æŒ‡å®šé¡¹ç›®çš„æƒé™"""
        try:
            project = Project.objects.get(id=project_id)
            # è¶…çº§ç”¨æˆ·å¯ä»¥è®¿é—®æ‰€æœ‰é¡¹ç›®
            if user.is_superuser:
                return project
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯é¡¹ç›®æˆå‘˜
            if ProjectMember.objects.filter(project=project, user=user).exists():
                return project
            return None
        except Project.DoesNotExist:
            return None

    def post(self, request, *args, **kwargs):
        """æ‰¹é‡åˆ é™¤èŠå¤©ä¼šè¯"""
        session_ids = request.data.get('session_ids', [])
        project_id = request.data.get('project_id')

        if not session_ids or not isinstance(session_ids, list):
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "session_ids must be a non-empty list.", "data": {},
                "errors": {"session_ids": ["This field is required and must be a list."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        if not project_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "project_id is required.", "data": {},
                "errors": {"project_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥é¡¹ç›®æƒé™
        project = self._check_project_permission(request.user, project_id)
        if not project:
            return Response({
                "status": "error", "code": status.HTTP_403_FORBIDDEN,
                "message": "You don't have permission to access this project or project doesn't exist.", "data": {},
                "errors": {"project_id": ["Permission denied or project not found."]}
            }, status=status.HTTP_403_FORBIDDEN)

        db_path = os.path.join(str(settings.BASE_DIR), "chat_history.sqlite")
        
        if not os.path.exists(db_path):
            return Response({
                "status": "success",
                "code": status.HTTP_200_OK,
                "message": "No chat history found to delete (history file does not exist).",
                "data": {"deleted_count": 0, "failed_sessions": []}
            }, status=status.HTTP_200_OK)

        conn = None
        total_deleted = 0
        failed_sessions = []
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            for session_id in session_ids:
                try:
                    # æ„å»ºthread_id
                    thread_id_parts = [str(request.user.id), str(project_id), str(session_id)]
                    thread_id = "_".join(thread_id_parts)

                    # åˆ é™¤å¯¹åº”çš„checkpoints
                    cursor.execute("DELETE FROM checkpoints WHERE thread_id = ?", (thread_id,))
                    deleted_count = cursor.rowcount
                    
                    if deleted_count > 0:
                        total_deleted += deleted_count
                        logger.info(f"Deleted {deleted_count} records for session_id: {session_id}")
                    else:
                        logger.warning(f"No records found for session_id: {session_id}")
                        failed_sessions.append({
                            "session_id": session_id,
                            "reason": "No records found"
                        })
                    
                    # åŒæ—¶åˆ é™¤Djangoä¸­çš„ChatSessionè®°å½•
                    try:
                        ChatSession.objects.filter(
                            session_id=session_id,
                            user=request.user,
                            project=project
                        ).delete()
                    except Exception as e:
                        logger.warning(f"Failed to delete ChatSession for {session_id}: {e}")
                        
                except Exception as e:
                    logger.error(f"Error deleting session {session_id}: {e}")
                    failed_sessions.append({
                        "session_id": session_id,
                        "reason": str(e)
                    })

            conn.commit()

            message = f"Successfully deleted {total_deleted} checkpoint records from {len(session_ids)} sessions."
            if failed_sessions:
                message += f" {len(failed_sessions)} sessions failed or had no records."

            return Response({
                "status": "success", "code": status.HTTP_200_OK,
                "message": message,
                "data": {
                    "deleted_count": total_deleted,
                    "processed_sessions": len(session_ids),
                    "failed_sessions": failed_sessions
                }
            }, status=status.HTTP_200_OK)

        except sqlite3.Error as e:
            logger.error(f"SQLite error during batch delete: {e}", exc_info=True)
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"Database error while batch deleting chat history: {str(e)}", "data": {},
                "errors": {"database_error": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except Exception as e:
            logger.error(f"Unexpected error during batch delete: {e}", exc_info=True)
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"An unexpected error occurred: {str(e)}", "data": {},
                "errors": {"unexpected_error": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        finally:
            if conn:
                conn.close()


class UserChatSessionsAPIView(APIView):
    """
    API endpoint for listing all chat session IDs for the authenticated user in a specific project.
    æ”¯æŒé¡¹ç›®éš”ç¦»ï¼Œåªè¿”å›æŒ‡å®šé¡¹ç›®çš„èŠå¤©ä¼šè¯ã€‚
    """
    permission_classes = [permissions.IsAuthenticated]

    def _check_project_permission(self, user, project_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®¿é—®æŒ‡å®šé¡¹ç›®çš„æƒé™"""
        try:
            project = Project.objects.get(id=project_id)
            # è¶…çº§ç”¨æˆ·å¯ä»¥è®¿é—®æ‰€æœ‰é¡¹ç›®
            if user.is_superuser:
                return project
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯é¡¹ç›®æˆå‘˜
            if ProjectMember.objects.filter(project=project, user=user).exists():
                return project
            return None
        except Project.DoesNotExist:
            return None

    def get(self, request, *args, **kwargs):
        user_id = str(request.user.id)
        project_id = request.query_params.get('project_id')

        if not project_id:
            return Response({
                "status": "error", "code": status.HTTP_400_BAD_REQUEST,
                "message": "project_id query parameter is required.", "data": {},
                "errors": {"project_id": ["This field is required."]}
            }, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥é¡¹ç›®æƒé™
        project = self._check_project_permission(request.user, project_id)
        if not project:
            return Response({
                "status": "error", "code": status.HTTP_403_FORBIDDEN,
                "message": "You don't have permission to access this project or project doesn't exist.", "data": {},
                "errors": {"project_id": ["Permission denied or project not found."]}
            }, status=status.HTTP_403_FORBIDDEN)

        db_path = os.path.join(str(settings.BASE_DIR), "chat_history.sqlite")
        session_ids = set() # Use a set to store unique session_ids

        if not os.path.exists(db_path):
            return Response({
                "status": "success",
                "code": status.HTTP_200_OK,
                "message": "No chat history found (history file does not exist).",
                "data": {"user_id": user_id, "sessions": []}
            }, status=status.HTTP_200_OK)

        conn = None
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Query for distinct thread_ids starting with the user_id and project_id prefix
            # The thread_id is stored as "USERID_PROJECTID_SESSIONID"
            thread_id_prefix = f"{user_id}_{project_id}_"
            cursor.execute("SELECT DISTINCT thread_id FROM checkpoints WHERE thread_id LIKE ?", (thread_id_prefix + '%',))

            rows = cursor.fetchall()

            for row in rows:
                full_thread_id = row[0]
                # Extract session_id part: everything after "USERID_PROJECTID_"
                if full_thread_id.startswith(thread_id_prefix):
                    session_id_part = full_thread_id[len(thread_id_prefix):]
                    if session_id_part: # Ensure there's something after the prefix
                        session_ids.add(session_id_part)

            return Response({
                "status": "success", "code": status.HTTP_200_OK,
                "message": "User chat sessions retrieved successfully.",
                "data": {
                    "user_id": user_id,
                    "project_id": project_id,
                    "project_name": project.name,
                    "sessions": sorted(list(session_ids))
                } # Return sorted list
            }, status=status.HTTP_200_OK)

        except sqlite3.Error as e:
            # import logging
            # logging.exception(f"SQLite error retrieving sessions for user {user_id}: {e}")
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"Database error while retrieving user sessions: {str(e)}", "data": {},
                "errors": {"database_error": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except Exception as e:
            # import logging
            # logging.exception(f"Unexpected error retrieving sessions for user {user_id}: {e}")
            return Response({
                "status": "error", "code": status.HTTP_500_INTERNAL_SERVER_ERROR,
                "message": f"An unexpected error occurred: {str(e)}", "data": {},
                "errors": {"unexpected_error": [str(e)]}
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        finally:
            if conn:
                conn.close()


from django.views import View
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt
from rest_framework_simplejwt.authentication import JWTAuthentication
from rest_framework.exceptions import AuthenticationFailed

@method_decorator(csrf_exempt, name='dispatch')
class ChatStreamAPIView(View):
    """
    API endpoint for streaming chat with the currently active LLM using LangGraph,
    with potential integration of remote MCP tools.
    æ”¯æŒé¡¹ç›®éš”ç¦»ï¼ŒèŠå¤©è®°å½•æŒ‰é¡¹ç›®åˆ†ç»„ã€‚
    ä½¿ç”¨Server-Sent Events (SSE)å®ç°æµå¼å“åº”ã€‚
    ä½¿ç”¨DjangoåŸç”ŸViewç»•è¿‡DRFçš„æ¸²æŸ“å™¨ç³»ç»Ÿã€‚
    """

    async def authenticate_request(self, request):
        """æ‰‹åŠ¨è¿›è¡ŒJWTè®¤è¯ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        auth_header = request.META.get('HTTP_AUTHORIZATION')
        if not auth_header or not auth_header.startswith('Bearer '):
            raise AuthenticationFailed('Authentication credentials were not provided.')

        token = auth_header.split(' ')[1]
        jwt_auth = JWTAuthentication()

        try:
            # åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨sync_to_asyncåŒ…è£…åŒæ­¥æ–¹æ³•
            validated_token = await sync_to_async(jwt_auth.get_validated_token)(token)
            user = await sync_to_async(jwt_auth.get_user)(validated_token)
            return user
        except Exception as e:
            raise AuthenticationFailed(f'Invalid token: {str(e)}')

    def _check_project_permission(self, user, project_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®¿é—®æŒ‡å®šé¡¹ç›®çš„æƒé™"""
        try:
            project = Project.objects.get(id=project_id)
            # è¶…çº§ç”¨æˆ·å¯ä»¥è®¿é—®æ‰€æœ‰é¡¹ç›®
            if user.is_superuser:
                return project
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯é¡¹ç›®æˆå‘˜
            if ProjectMember.objects.filter(project=project, user=user).exists():
                return project
            return None
        except Project.DoesNotExist:
            return None

    async def _create_sse_generator(self, request, user_message_content, session_id, project_id, project,
                                   knowledge_base_id=None, use_knowledge_base=True, similarity_threshold=0.7, top_k=5, prompt_id=None, image_base64=None):
        """åˆ›å»ºSSEæ•°æ®ç”Ÿæˆå™¨"""
        try:
            # è·å–æ´»è·ƒçš„LLMé…ç½®
            active_config = await sync_to_async(LLMConfig.objects.get)(is_active=True)
            logger.info(f"ChatStreamAPIView: Using active LLMConfig: {active_config.name}")
        except LLMConfig.DoesNotExist:
            yield f"data: {json.dumps({'type': 'error', 'message': 'No active LLM configuration found'})}\n\n"
            return
        except LLMConfig.MultipleObjectsReturned:
            yield f"data: {json.dumps({'type': 'error', 'message': 'Multiple active LLM configurations found'})}\n\n"
            return

        # éªŒè¯å›¾ç‰‡è¾“å…¥æ˜¯å¦æ”¯æŒ
        if image_base64 and not active_config.supports_vision:
            logger.warning(f"ChatStreamAPIView: Image input rejected - model {active_config.name} does not support vision")
            yield f"data: {json.dumps({'type': 'error', 'message': f'å½“å‰æ¨¡å‹ {active_config.name} ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼Œè¯·åˆ‡æ¢åˆ°æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4Vã€Claude 3ã€Gemini Vision æˆ– Qwen-VLï¼‰'})}\n\n"
            return

        try:
            # ä½¿ç”¨æ–°çš„LLMå·¥å‚å‡½æ•°ï¼Œæ”¯æŒå¤šä¾›åº”å•†
            llm = create_llm_instance(active_config, temperature=0.7)
            logger.info(f"ChatStreamAPIView: Initialized LLM with provider auto-detection")

            db_path = os.path.join(str(settings.BASE_DIR), "chat_history.sqlite")
            async with AsyncSqliteSaver.from_conn_string(db_path) as actual_memory_checkpointer:
                # åŠ è½½è¿œç¨‹MCPå·¥å…·
                logger.info("ChatStreamAPIView: Attempting to load remote MCP tools.")
                mcp_tools_list = []
                try:
                    active_remote_mcp_configs_qs = RemoteMCPConfig.objects.filter(is_active=True)
                    active_remote_mcp_configs = await sync_to_async(list)(active_remote_mcp_configs_qs)

                    if active_remote_mcp_configs:
                        client_mcp_config = {}
                        for r_config in active_remote_mcp_configs:
                            config_key = r_config.name or f"remote_config_{r_config.id}"
                            client_mcp_config[config_key] = {
                                "url": r_config.url,
                                "transport": (r_config.transport or "streamable_http").replace('-', '_'),
                            }
                            if r_config.headers and isinstance(r_config.headers, dict) and r_config.headers:
                                client_mcp_config[config_key]["headers"] = r_config.headers

                        if client_mcp_config:
                            logger.info(f"ChatStreamAPIView: Initializing persistent MCP client with config: {client_mcp_config}")
                            # ä½¿ç”¨æŒä¹…åŒ–MCPä¼šè¯ç®¡ç†å™¨ï¼Œä¼ é€’ç”¨æˆ·ã€é¡¹ç›®å’Œä¼šè¯ä¿¡æ¯ä»¥æ”¯æŒè·¨å¯¹è¯è½®æ¬¡çš„çŠ¶æ€ä¿æŒ
                            mcp_tools_list = await mcp_session_manager.get_tools_for_config(
                                client_mcp_config,
                                user_id=str(request.user.id),
                                project_id=str(project_id),
                                session_id=session_id  # ä¼ é€’session_idä»¥å¯ç”¨ä¼šè¯çº§åˆ«çš„å·¥å…·ç¼“å­˜
                            )
                            logger.info(f"ChatStreamAPIView: Successfully loaded {len(mcp_tools_list)} persistent tools from remote MCP servers")
                        else:
                            logger.info("ChatStreamAPIView: No active remote MCP configurations to build client config.")
                    else:
                        logger.info("ChatStreamAPIView: No active RemoteMCPConfig found.")
                except Exception as e:
                    logger.error(f"ChatStreamAPIView: Error loading remote MCP tools: {e}", exc_info=True)
                    yield f"data: {json.dumps({'type': 'warning', 'message': f'Failed to load MCP tools: {str(e)}'})}\n\n"

                # å‡†å¤‡LangGraph runnable
                runnable_to_invoke = None

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºAgentï¼ˆæœ‰MCPå·¥å…·ï¼‰
                if mcp_tools_list:
                    logger.info(f"ChatStreamAPIView: Attempting to create agent with {len(mcp_tools_list)} remote tools.")
                    try:
                        # å¦‚æœåŒæ—¶æœ‰çŸ¥è¯†åº“å’ŒMCPå·¥å…·ï¼Œåˆ›å»ºçŸ¥è¯†åº“å¢å¼ºçš„Agent
                        if knowledge_base_id and use_knowledge_base:
                            logger.info(f"ChatStreamAPIView: Creating knowledge-enhanced agent with {len(mcp_tools_list)} tools and knowledge base {knowledge_base_id}")

                            # åˆ›å»ºçŸ¥è¯†åº“å·¥å…·
                            from knowledge.langgraph_integration import create_knowledge_tool
                            knowledge_tool = create_knowledge_tool(
                                knowledge_base_id=knowledge_base_id,
                                user=request.user,
                                similarity_threshold=similarity_threshold,
                                top_k=top_k
                            )

                            # å°†çŸ¥è¯†åº“å·¥å…·æ·»åŠ åˆ°MCPå·¥å…·åˆ—è¡¨
                            enhanced_tools = mcp_tools_list + [knowledge_tool]
                            agent_executor = create_react_agent(llm, enhanced_tools, checkpointer=actual_memory_checkpointer)
                            runnable_to_invoke = agent_executor
                            logger.info(f"ChatStreamAPIView: Knowledge-enhanced agent created with {len(enhanced_tools)} tools (including knowledge base)")
                            yield create_sse_data({'type': 'info', 'message': f'Knowledge-enhanced agent initialized with {len(enhanced_tools)} tools'})
                        else:
                            # åªæœ‰MCPå·¥å…·ï¼Œåˆ›å»ºæ™®é€šAgent
                            agent_executor = create_react_agent(llm, mcp_tools_list, checkpointer=actual_memory_checkpointer)
                            runnable_to_invoke = agent_executor
                            logger.info("ChatStreamAPIView: Agent with remote tools created with checkpointer.")
                            yield create_sse_data({'type': 'info', 'message': f'Agent initialized with {len(mcp_tools_list)} tools'})
                    except Exception as e:
                        logger.error(f"ChatStreamAPIView: Failed to create agent with remote tools: {e}. Falling back to knowledge-enhanced chatbot.", exc_info=True)
                        yield create_sse_data({'type': 'warning', 'message': 'Failed to create agent with tools, using knowledge-enhanced chatbot'})

                if not runnable_to_invoke:
                    logger.info("ChatStreamAPIView: No remote tools or agent creation failed. Using knowledge-enhanced chatbot.")

                    def knowledge_enhanced_chatbot_node(state: AgentState):
                        """çŸ¥è¯†åº“å¢å¼ºçš„èŠå¤©æœºå™¨äººèŠ‚ç‚¹"""
                        messages = state['messages']
                        if not messages:
                            return {"messages": []}

                        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                        last_message = messages[-1]
                        if hasattr(last_message, 'content'):
                            user_query = last_message.content
                        else:
                            user_query = str(last_message)

                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨çŸ¥è¯†åº“
                        if knowledge_base_id and use_knowledge_base:
                            try:
                                # ä½¿ç”¨çŸ¥è¯†åº“å¢å¼ºå›ç­”
                                from knowledge.langgraph_integration import KnowledgeRAGService
                                rag_service = KnowledgeRAGService(llm)
                                rag_result = rag_service.query(
                                    question=user_query,
                                    knowledge_base_id=knowledge_base_id,
                                    user=request.user,
                                    similarity_threshold=similarity_threshold,
                                    top_k=top_k
                                )

                                # ä½¿ç”¨RAGç»“æœä½œä¸ºä¸Šä¸‹æ–‡
                                context_prompt = f"åŸºäºä»¥ä¸‹ç›¸å…³ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n{rag_result['context']}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_query}"
                                enhanced_messages = messages[:-1] + [HumanMessage(content=context_prompt)]
                                invoked_response = llm.invoke(enhanced_messages)
                                logger.info(f"ChatStreamAPIView: Used knowledge base {knowledge_base_id} for enhanced response")

                            except Exception as e:
                                logger.warning(f"ChatStreamAPIView: Knowledge base query failed: {e}, falling back to normal response")
                                invoked_response = llm.invoke(messages)
                        else:
                            # æ™®é€šèŠå¤©å›å¤
                            invoked_response = llm.invoke(messages)

                        return {"messages": [invoked_response]}

                    graph_builder = StateGraph(AgentState)
                    graph_builder.add_node("chatbot", knowledge_enhanced_chatbot_node)
                    graph_builder.set_entry_point("chatbot")
                    graph_builder.add_edge("chatbot", END)
                    runnable_to_invoke = graph_builder.compile(checkpointer=actual_memory_checkpointer)

                    if knowledge_base_id and use_knowledge_base:
                        logger.info(f"ChatStreamAPIView: Knowledge-enhanced chatbot initialized with KB: {knowledge_base_id}")
                        yield create_sse_data({'type': 'info', 'message': f'Knowledge-enhanced chatbot initialized with knowledge base'})
                    else:
                        logger.info("ChatStreamAPIView: Basic chatbot initialized")
                        yield create_sse_data({'type': 'info', 'message': 'Basic chatbot initialized'})

                # ç¡®å®šthread_id - åŒ…å«é¡¹ç›®IDä»¥å®ç°é¡¹ç›®éš”ç¦»
                thread_id_parts = [str(request.user.id), str(project_id)]
                if session_id:
                    thread_id_parts.append(str(session_id))
                thread_id = "_".join(thread_id_parts)
                logger.info(f"ChatStreamAPIView: Using thread_id: {thread_id} for project: {project.name}")

                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯
                messages_list = []

                # è·å–æœ‰æ•ˆçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨æˆ·æç¤ºè¯ä¼˜å…ˆï¼‰
                effective_prompt, prompt_source = await get_effective_system_prompt_async(request.user, prompt_id)
                logger.info(f"ChatStreamAPIView: Using {prompt_source} prompt: {repr(effective_prompt[:100] if effective_prompt else None)}")

                # æ£€æŸ¥å½“å‰ä¼šè¯æ˜¯å¦å·²ç»æœ‰ç³»ç»Ÿæç¤ºè¯
                should_add_system_prompt = False
                if effective_prompt:
                    try:
                        # å°è¯•è·å–å½“å‰ä¼šè¯çš„å†å²æ¶ˆæ¯ - ä½¿ç”¨å¼‚æ­¥æ¥å£
                        checkpoint_tuples_list = []
                        async for checkpoint_tuple in actual_memory_checkpointer.alist(config={"configurable": {"thread_id": thread_id}}):
                            checkpoint_tuples_list.append(checkpoint_tuple)

                        if checkpoint_tuples_list:
                            # æ£€æŸ¥æœ€æ–°checkpointä¸­æ˜¯å¦å·²æœ‰ç³»ç»Ÿæç¤ºè¯
                            latest_checkpoint = checkpoint_tuples_list[0].checkpoint
                            if (latest_checkpoint and 'channel_values' in latest_checkpoint
                                and 'messages' in latest_checkpoint['channel_values']):
                                existing_messages = latest_checkpoint['channel_values']['messages']
                                # æ£€æŸ¥ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯ç³»ç»Ÿæ¶ˆæ¯
                                if not existing_messages or not isinstance(existing_messages[0], SystemMessage):
                                    should_add_system_prompt = True
                            else:
                                should_add_system_prompt = True
                        else:
                            # æ–°ä¼šè¯ï¼Œéœ€è¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯
                            should_add_system_prompt = True
                    except Exception as e:
                        logger.warning(f"ChatStreamAPIView: Error checking existing messages: {e}")
                        should_add_system_prompt = True

                    if should_add_system_prompt:
                        messages_list.append(SystemMessage(content=effective_prompt))
                        logger.info(f"ChatStreamAPIView: Added {prompt_source} system prompt: {effective_prompt[:100]}...")
                else:
                    logger.info("ChatStreamAPIView: No system prompt available")

                # éªŒè¯ç”¨æˆ·æ¶ˆæ¯å†…å®¹ä¸ä¸ºç©º
                if not user_message_content or not user_message_content.strip():
                    logger.error("ChatStreamAPIView: User message content is empty or whitespace only")
                    yield create_sse_data({'type': 'error', 'message': 'User message content cannot be empty'})
                    return

                # ç¡®ä¿ç”¨æˆ·æ¶ˆæ¯å†…å®¹æ ¼å¼æ­£ç¡®
                clean_user_message = user_message_content.strip()
                if not clean_user_message and not image_base64:
                    logger.error("ChatStreamAPIView: User message is empty after stripping")
                    yield create_sse_data({'type': 'error', 'message': 'User message cannot be empty'})
                    return

                # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
                if image_base64:
                    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œåˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
                    human_message_content = [
                        {"type": "text", "text": clean_user_message},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                        }
                    ]
                    messages_list.append(HumanMessage(content=human_message_content))
                    logger.info(f"ChatStreamAPIView: Added multimodal message with image")
                else:
                    # çº¯æ–‡æœ¬æ¶ˆæ¯
                    messages_list.append(HumanMessage(content=clean_user_message))
                logger.info(f"ChatStreamAPIView: Final messages list length: {len(messages_list)}")

                # éªŒè¯æ¶ˆæ¯åˆ—è¡¨ä¸ä¸ºç©ºä¸”æ‰€æœ‰æ¶ˆæ¯éƒ½æœ‰æœ‰æ•ˆå†…å®¹
                if not messages_list:
                    logger.error("ChatStreamAPIView: Messages list is empty")
                    yield create_sse_data({'type': 'error', 'message': 'No valid messages to process'})
                    return

                for i, msg in enumerate(messages_list):
                    if not hasattr(msg, 'content') or not msg.content or not str(msg.content).strip():
                        logger.error(f"ChatStreamAPIView: Message at index {i} has empty content: {msg}")
                        yield create_sse_data({'type': 'error', 'message': f'Message at index {i} has invalid content'})
                        return
                    logger.info(f"ChatStreamAPIView: Message {i}: {type(msg).__name__} with content length {len(str(msg.content))}")

                input_messages = {"messages": messages_list}
                invoke_config = {
                    "configurable": {"thread_id": thread_id},
                    "recursion_limit": 100  # å¢åŠ é€’å½’é™åˆ¶ï¼Œæ”¯æŒç”Ÿæˆæ›´å¤šæµ‹è¯•ç”¨ä¾‹
                }
                logger.info(f"ChatStreamAPIView: Set recursion_limit to 100 for thread_id: {thread_id}")
                logger.info(f"ChatStreamAPIView: Input messages structure: {input_messages}")

                # è¯¦ç»†è®°å½•æ¯ä¸ªæ¶ˆæ¯çš„å†…å®¹
                for i, msg in enumerate(messages_list):
                    logger.info(f"ChatStreamAPIView: Message {i}: type={type(msg).__name__}, content={repr(msg.content)}")

                # å‘é€å¼€å§‹ä¿¡å·
                yield create_sse_data({'type': 'start', 'thread_id': thread_id, 'session_id': session_id, 'project_id': project_id})

                # ä½¿ç”¨astreamè¿›è¡Œæµå¼å¤„ç†ï¼Œæ”¯æŒå¤šç§æ¨¡å¼
                stream_modes = ["updates", "messages"]

                try:
                    async for stream_mode, chunk in runnable_to_invoke.astream(
                        input_messages,
                        config=invoke_config,
                        stream_mode=stream_modes
                    ):
                        if stream_mode == "updates":
                            # ä»£ç†è¿›åº¦æ›´æ–° - å®‰å…¨åœ°åºåˆ—åŒ–å¤æ‚å¯¹è±¡
                            try:
                                # å°è¯•å°†chunkè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                                if hasattr(chunk, '__dict__'):
                                    serializable_chunk = str(chunk)
                                else:
                                    serializable_chunk = chunk
                                yield create_sse_data({'type': 'update', 'data': serializable_chunk})
                            except (TypeError, ValueError) as e:
                                yield create_sse_data({'type': 'update', 'data': f'Update: {str(chunk)}'})
                        elif stream_mode == "messages":
                            # LLMä»¤ç‰Œæµå¼ä¼ è¾“
                            if hasattr(chunk, 'content') and chunk.content:
                                yield create_sse_data({'type': 'message', 'data': chunk.content})
                            else:
                                yield create_sse_data({'type': 'message', 'data': str(chunk)})

                        # æ·»åŠ å°å»¶è¿Ÿä»¥ç¡®ä¿æµå¼ä¼ è¾“æ•ˆæœ
                        await asyncio.sleep(0.01)

                except Exception as e:
                    logger.error(f"ChatStreamAPIView: Error during streaming: {e}", exc_info=True)
                    yield create_sse_data({'type': 'error', 'message': f'Streaming error: {str(e)}'})

                # å‘é€å®Œæˆä¿¡å·
                yield create_sse_data({'type': 'complete'})

                # å‘é€æµç»“æŸæ ‡è®°
                yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"ChatStreamAPIView: Error in stream generator: {e}", exc_info=True)
            yield create_sse_data({'type': 'error', 'message': f'Generator error: {str(e)}'})

    async def post(self, request, *args, **kwargs):
        """å¤„ç†æµå¼èŠå¤©è¯·æ±‚"""
        try:
            # æ‰‹åŠ¨è®¤è¯ï¼ˆå¼‚æ­¥ï¼‰
            user = await self.authenticate_request(request)
            request.user = user
            logger.info(f"ChatStreamAPIView: Received POST request from user {user.id}")
        except AuthenticationFailed as e:
            error_data = create_sse_data({
                'type': 'error',
                'message': str(e),
                'code': 401
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=401
            )

        # è§£æJSONæ•°æ®
        try:
            import json as json_module
            body_data = json_module.loads(request.body.decode('utf-8'))
        except (json_module.JSONDecodeError, UnicodeDecodeError) as e:
            error_data = create_sse_data({
                'type': 'error',
                'message': f'Invalid JSON data: {str(e)}',
                'code': 400
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=400
            )

        user_message_content = body_data.get('message')
        session_id = body_data.get('session_id')
        project_id = body_data.get('project_id')
        image_base64 = body_data.get('image')  # å›¾ç‰‡base64ç¼–ç ï¼ˆä¸å«å‰ç¼€ï¼‰

        # çŸ¥è¯†åº“ç›¸å…³å‚æ•°
        knowledge_base_id = body_data.get('knowledge_base_id')
        use_knowledge_base = body_data.get('use_knowledge_base', True)
        similarity_threshold = body_data.get('similarity_threshold', 0.7)
        top_k = body_data.get('top_k', 5)

        # æç¤ºè¯ç›¸å…³å‚æ•°
        prompt_id = body_data.get('prompt_id')  # ç”¨æˆ·æŒ‡å®šçš„æç¤ºè¯ID

        # éªŒè¯é¡¹ç›®IDæ˜¯å¦æä¾›
        if not project_id:
            error_data = create_sse_data({
                'type': 'error',
                'message': 'project_id is required',
                'code': status.HTTP_400_BAD_REQUEST
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=status.HTTP_400_BAD_REQUEST
            )

        # æ£€æŸ¥é¡¹ç›®æƒé™
        project = await sync_to_async(self._check_project_permission)(request.user, project_id)
        if not project:
            error_data = create_sse_data({
                'type': 'error',
                'message': "You don't have permission to access this project or project doesn't exist",
                'code': status.HTTP_403_FORBIDDEN
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=status.HTTP_403_FORBIDDEN
            )

        is_new_session = False
        if not session_id:
            session_id = uuid.uuid4().hex
            is_new_session = True
            logger.info(f"ChatStreamAPIView: Generated new session_id: {session_id}")

        # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œç«‹å³åˆ›å»ºChatSessionå¯¹è±¡
        if is_new_session:
            try:
                await sync_to_async(ChatSession.objects.create)(
                    user=request.user,
                    session_id=session_id,
                    project=project,
                    title=f"æ–°å¯¹è¯ - {user_message_content[:30]}" # ä½¿ç”¨æ¶ˆæ¯å†…å®¹ä½œä¸ºä¸´æ—¶æ ‡é¢˜
                )
                logger.info(f"ChatStreamAPIView: Created new ChatSession entry for session_id: {session_id}")
            except Exception as e:
                logger.error(f"ChatStreamAPIView: Failed to create ChatSession entry: {e}", exc_info=True)


        if not user_message_content:
            logger.warning("ChatStreamAPIView: Message content is required but not provided.")
            error_data = create_sse_data({
                'type': 'error',
                'message': 'Message content is required',
                'code': status.HTTP_400_BAD_REQUEST
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=status.HTTP_400_BAD_REQUEST
            )

        # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨
        async def async_generator():
            async for chunk in self._create_sse_generator(
                request, user_message_content, session_id, project_id, project,
                knowledge_base_id, use_knowledge_base, similarity_threshold, top_k, prompt_id, image_base64
            ):
                yield chunk

        response = StreamingHttpResponse(
            async_generator(),
            content_type='text/event-stream; charset=utf-8'
        )
        response['Cache-Control'] = 'no-cache'
        response['Access-Control-Allow-Origin'] = '*'
        response['Access-Control-Allow-Headers'] = 'Cache-Control'

        return response


class ProviderChoicesAPIView(APIView):
    """è·å–å¯ç”¨çš„LLMä¾›åº”å•†é€‰é¡¹"""
    permission_classes = [IsAuthenticated]
    
    def get(self, request, *args, **kwargs):
        """è¿”å›æ‰€æœ‰å¯ç”¨çš„ä¾›åº”å•†é€‰é¡¹"""
        choices = [{'value': choice[0], 'label': choice[1]} for choice in LLMConfig.PROVIDER_CHOICES]
        return Response({
            'status': 'success',
            'code': status.HTTP_200_OK,
            'message': 'Provider choices retrieved successfully.',
            'data': {'choices': choices}
        })


class KnowledgeRAGAPIView(APIView):
    """çŸ¥è¯†åº“RAGæŸ¥è¯¢APIè§†å›¾"""
    permission_classes = [IsAuthenticated]

    def post(self, request, *args, **kwargs):
        """æ‰§è¡ŒçŸ¥è¯†åº“RAGæŸ¥è¯¢"""
        try:
            # è·å–è¯·æ±‚å‚æ•°
            query = request.data.get('query')
            knowledge_base_id = request.data.get('knowledge_base_id')
            project_id = request.data.get('project_id')

            if not query:
                return Response(
                    {'error': 'æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º'},
                    status=status.HTTP_400_BAD_REQUEST
                )

            if not knowledge_base_id:
                return Response(
                    {'error': 'çŸ¥è¯†åº“IDä¸èƒ½ä¸ºç©º'},
                    status=status.HTTP_400_BAD_REQUEST
                )

            # éªŒè¯é¡¹ç›®æƒé™
            if project_id:
                try:
                    project = Project.objects.get(id=project_id)
                    if not project.members.filter(user=request.user).exists() and not request.user.is_superuser:
                        return Response(
                            {'error': 'æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤é¡¹ç›®'},
                            status=status.HTTP_403_FORBIDDEN
                        )
                except Project.DoesNotExist:
                    return Response(
                        {'error': 'é¡¹ç›®ä¸å­˜åœ¨'},
                        status=status.HTTP_404_NOT_FOUND
                    )

            # éªŒè¯çŸ¥è¯†åº“æƒé™
            try:
                knowledge_base = KnowledgeBase.objects.get(id=knowledge_base_id)
                if not knowledge_base.project.members.filter(user=request.user).exists() and not request.user.is_superuser:
                    return Response(
                        {'error': 'æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤çŸ¥è¯†åº“'},
                        status=status.HTTP_403_FORBIDDEN
                    )
            except KnowledgeBase.DoesNotExist:
                return Response(
                    {'error': 'çŸ¥è¯†åº“ä¸å­˜åœ¨'},
                    status=status.HTTP_404_NOT_FOUND
                )

            # è·å–LLMé…ç½®
            try:
                active_config = LLMConfig.objects.filter(is_active=True).first()
                if not active_config:
                    return Response(
                        {'error': 'æ²¡æœ‰å¯ç”¨çš„LLMé…ç½®'},
                        status=status.HTTP_500_INTERNAL_SERVER_ERROR
                    )

                # ä½¿ç”¨æ–°çš„LLMå·¥å‚å‡½æ•°ï¼Œæ”¯æŒå¤šä¾›åº”å•†
                llm = create_llm_instance(active_config, temperature=0.7)
            except Exception as e:
                logger.error(f"LLMé…ç½®é”™è¯¯: {e}")
                return Response(
                    {'error': 'LLMé…ç½®é”™è¯¯'},
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR
                )

            # æ‰§è¡ŒRAGæŸ¥è¯¢
            rag_service = KnowledgeRAGService(llm)
            result = rag_service.query(
                question=query,
                knowledge_base_id=knowledge_base_id,
                user=request.user
            )

            return Response({
                'query': result['question'],
                'answer': result['answer'],
                'sources': result['context'],
                'retrieval_time': result['retrieval_time'],
                'generation_time': result['generation_time'],
                'total_time': result['total_time']
            })

        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“RAGæŸ¥è¯¢å¤±è´¥: {e}")
            return Response(
                {'error': f'æŸ¥è¯¢å¤±è´¥: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )